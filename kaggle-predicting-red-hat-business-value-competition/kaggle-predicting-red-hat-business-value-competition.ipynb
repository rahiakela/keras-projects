{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#       Predicting Red Hat Business Value \n",
    "#############################################################\n",
    "\"\"\"\n",
    "Reference: https://www.kaggle.com/c/predicting-red-hat-business-value\n",
    "\n",
    "The organization is an American multinational software company that provides open source software products to the \n",
    "enterprise community.Their primary product is Red Hat Enterprise Linux, the most popular distribution of Linux OS, \n",
    "used by various large enterprises. In its services, it helps organizations align their IT strategies by providing \n",
    "enterprise-grade solutions through an open business model and an affordable, predictable subscription model. \n",
    "These subscriptions from large enterprise customers create a substantial part of their revenue, and therefore it is \n",
    "of paramount importance for them to understand their valuable customers and serve them better by prioritizing \n",
    "resources and strategies to drive improved business value.\n",
    "\n",
    "How Can We Identify a Potential Customer?\n",
    "Red Hat has been in existence for over 25 years. In the long stint of business, they have accumulated and captured \n",
    "a vast amount of data from customer interactions and their descriptive attributes. This rich source of data could \n",
    "be a gold mine of patterns that can help in identifying a potential customer by studying the vast and complex \n",
    "historical patterns in the interaction data.\n",
    "With the ever-growing popularity and prowess of DL, we can develop a DNN that can learn from historic customer \n",
    "attributes and operational interaction data to understand the deep patterns and predict whether a new customer will\n",
    "potentially be a high-value customer for various business services.\n",
    "Therefore, we will develop and train a DNN to learn the chances that a customer will be a potential high-value \n",
    "customer, using various customer attributes and operational interaction attributes.\n",
    "\"\"\"\n",
    "# Exploring the Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 2 datasets provided in the Zip Folder\n",
    "act_train = pd.read_csv('D:\\\\ml-data\\\\predicting-red-hat-business-value\\\\act_train.csv')\n",
    "people = pd.read_csv('D:\\\\ml-data\\\\predicting-red-hat-business-value\\\\people.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DF: (2197291, 15)\n",
      "Shape of People DF: (189118, 41)\n"
     ]
    }
   ],
   "source": [
    "# Explore the shape of the datasets\n",
    "print('Shape of DF:', act_train.shape)\n",
    "print('Shape of People DF:', people.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>date</th>\n",
       "      <th>activity_category</th>\n",
       "      <th>char_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>char_5</th>\n",
       "      <th>char_6</th>\n",
       "      <th>char_7</th>\n",
       "      <th>char_8</th>\n",
       "      <th>char_9</th>\n",
       "      <th>char_10</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_1734928</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_2434093</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_3404049</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_3651215</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_4109017</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  people_id   activity_id        date activity_category char_1 char_2 char_3  \\\n",
       "0   ppl_100  act2_1734928  2023-08-26            type 4    NaN    NaN    NaN   \n",
       "1   ppl_100  act2_2434093  2022-09-27            type 2    NaN    NaN    NaN   \n",
       "2   ppl_100  act2_3404049  2022-09-27            type 2    NaN    NaN    NaN   \n",
       "3   ppl_100  act2_3651215  2023-08-04            type 2    NaN    NaN    NaN   \n",
       "4   ppl_100  act2_4109017  2023-08-26            type 2    NaN    NaN    NaN   \n",
       "\n",
       "  char_4 char_5 char_6 char_7 char_8 char_9  char_10  outcome  \n",
       "0    NaN    NaN    NaN    NaN    NaN    NaN  type 76        0  \n",
       "1    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "2    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "3    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "4    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the contents of the first dataset\n",
    "act_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people_id                  0\n",
       "activity_id                0\n",
       "date                       0\n",
       "activity_category          0\n",
       "char_1               2039676\n",
       "char_2               2039676\n",
       "char_3               2039676\n",
       "char_4               2039676\n",
       "char_5               2039676\n",
       "char_6               2039676\n",
       "char_7               2039676\n",
       "char_8               2039676\n",
       "char_9               2039676\n",
       "char_10               157615\n",
       "outcome                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exploring the contents of the training dataset, we can see that it mostly has customer interaction data but is \n",
    "completely anonymized. Given the confidentiality of customers and their attributes, the entire data is anonymized,\n",
    "and this leaves us with little knowledge about its true nature. This is a common problem in data science. Quite \n",
    "often, the team that develops DL models faces the challenge of the data confidentiality of the end customer and is \n",
    "therefore provided only anonymized and sometimes encrypted data. This still shouldn’t be a roadblock. It is \n",
    "definitely best to have a data dictionary and complete understanding of the dataset, but nevertheless, we can still\n",
    "develop models with the provided information.\n",
    "\"\"\"\n",
    "# Calculating the % of Null values in each column for activity data\n",
    "act_train.isnull().sum()  # show sum of null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2197291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_train.shape[0]  # show total row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people_id            0.000000\n",
       "activity_id          0.000000\n",
       "date                 0.000000\n",
       "activity_category    0.000000\n",
       "char_1               0.928268\n",
       "char_2               0.928268\n",
       "char_3               0.928268\n",
       "char_4               0.928268\n",
       "char_5               0.928268\n",
       "char_6               0.928268\n",
       "char_7               0.928268\n",
       "char_8               0.928268\n",
       "char_9               0.928268\n",
       "char_10              0.071732\n",
       "outcome              0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate percentage of null by dividing total null by total row count\n",
    "act_train.isnull().sum() / act_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>char_1</th>\n",
       "      <th>group_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>date</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>char_5</th>\n",
       "      <th>char_6</th>\n",
       "      <th>char_7</th>\n",
       "      <th>...</th>\n",
       "      <th>char_29</th>\n",
       "      <th>char_30</th>\n",
       "      <th>char_31</th>\n",
       "      <th>char_32</th>\n",
       "      <th>char_33</th>\n",
       "      <th>char_34</th>\n",
       "      <th>char_35</th>\n",
       "      <th>char_36</th>\n",
       "      <th>char_37</th>\n",
       "      <th>char_38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100002</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 8688</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>type 28</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100003</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 33592</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>type 4</td>\n",
       "      <td>type 8</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppl_100004</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 22593</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>type 40</td>\n",
       "      <td>type 25</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 4</td>\n",
       "      <td>type 16</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl_100006</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 6534</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>type 40</td>\n",
       "      <td>type 25</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 8</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    people_id  char_1      group_1  char_2        date   char_3   char_4  \\\n",
       "0     ppl_100  type 2  group 17304  type 2  2021-06-29   type 5   type 5   \n",
       "1  ppl_100002  type 2   group 8688  type 3  2021-01-06  type 28   type 9   \n",
       "2  ppl_100003  type 2  group 33592  type 3  2022-06-10   type 4   type 8   \n",
       "3  ppl_100004  type 2  group 22593  type 3  2022-07-20  type 40  type 25   \n",
       "4  ppl_100006  type 2   group 6534  type 3  2022-07-27  type 40  type 25   \n",
       "\n",
       "   char_5  char_6   char_7  ... char_29 char_30  char_31  char_32  char_33  \\\n",
       "0  type 5  type 3  type 11  ...   False    True     True    False    False   \n",
       "1  type 5  type 3  type 11  ...   False    True     True     True     True   \n",
       "2  type 5  type 2   type 5  ...   False   False     True     True     True   \n",
       "3  type 9  type 4  type 16  ...    True    True     True     True     True   \n",
       "4  type 9  type 3   type 8  ...   False   False     True    False    False   \n",
       "\n",
       "   char_34  char_35  char_36  char_37  char_38  \n",
       "0     True     True     True    False       36  \n",
       "1     True     True     True    False       76  \n",
       "2     True    False     True     True       99  \n",
       "3     True     True     True     True       76  \n",
       "4    False     True     True    False       84  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Around nine features have more than 90% null values. We can’t do much to fix these features. Let’s move ahead\n",
    "and have a look at the people dataset.\n",
    "\"\"\"\n",
    "# Explore the contents of People dataset\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let’s check how many missing data points the customer dataset has. Since the customer dataset has around 40+ \n",
    "features, we can combine the missing value percentages for all columns together with the preceding code, \n",
    "instead of looking at each column individually.\n",
    "\"\"\"\n",
    "# Calculate the % of null values in for the entire dataset\n",
    "people.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to remove: ['char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9']\n",
      "Shape of DF: (2197291, 6)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "And we see that none of the columns in the customer dataset has missing values.\n",
    "\n",
    "To create a consolidated dataset, we need to join the activity and customer data on the people_id key. But before\n",
    "we do that, we need to take care of a few things. We need to drop the columns in the activity data that have 90%\n",
    "missing values, as they cannot be fixed. Secondly, the “date” and “char_10” columns are present in both datasets.\n",
    "In order to avoid a name clash, let us rename the “date” column in the activity dataset to “activity_date” and \n",
    "“char_10” in the activity data as “activity_type.” Next, we also need to fix the missing values in the \n",
    "“activity_type” column. Once these two tasks are accomplished, we will join the two datasets and explore the\n",
    "consolidated data.\n",
    "\"\"\"\n",
    "# Create the list of columns to drop from activity data\n",
    "columns_to_remove = ['char_' + str(x) for x in np.arange(1, 10)]\n",
    "print('Columns to remove:', columns_to_remove)\n",
    "\n",
    "# Remove the columns from the activity data\n",
    "act_train = act_train[list(set(act_train.columns) - set(columns_to_remove))]\n",
    "\n",
    "# Rename the 2 columns to avoid name clashes in merged data\n",
    "act_train = act_train.rename(columns={'date': 'activity_date', 'char_10': 'activity_type'})\n",
    "\n",
    "# Replace nulls in the activity_type column with the mode\n",
    "act_train['activity_type'] = act_train['activity_type'].fillna(act_train['activity_type'].mode()[0])\n",
    "\n",
    "# Print the shape of the final activity dataset\n",
    "print('Shape of DF:', act_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before merging: (2197291, 6)\n",
      "Shape after merging : (2197291, 46)\n"
     ]
    }
   ],
   "source": [
    "# We can now join the two datasets to create a consolidate activity and customer attributes dataset.\n",
    "# Merge the 2 datasets on 'people_id' key\n",
    "merged_df = act_train.merge(people, on=['people_id'], how='inner')\n",
    "print('Shape before merging:', act_train.shape)\n",
    "print('Shape after merging :', merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for outcome: [0 1]\n",
      "\n",
      "Percentage of distribution for outcome-\n",
      "0    0.556046\n",
      "1    0.443954\n",
      "Name: outcome, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let us now study the target (i.e., the variable we want to predict), named “outcome” in the dataset. We can check \n",
    "the distribution between potential vs. nonpotential customers.\n",
    "\"\"\"\n",
    "print('Unique values for outcome:', merged_df['outcome'].unique())\n",
    "print('\\nPercentage of distribution for outcome-')\n",
    "print(merged_df['outcome'].value_counts() / merged_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct DataTypes:  [dtype('O'), dtype('int64'), dtype('bool')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can see that there is a good mix in the distribution of potential customers, as around 45% are potential \n",
    "customers.\n",
    "\"\"\"\n",
    "\n",
    "###########################################\n",
    "#        Data Engineering\n",
    "###########################################\n",
    "\"\"\"\n",
    "Next, given that we have 45 columns altogether to explore and transform, let’s expedite the process by automating\n",
    "a few things.\n",
    "\"\"\"\n",
    "# Checking the distinct datatypes in the dataset.\n",
    "print('Distinct DataTypes: ', list(merged_df.dtypes.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean columns - \n",
      " ['char_10' 'char_11' 'char_12' 'char_13' 'char_14' 'char_15' 'char_16'\n",
      " 'char_17' 'char_18' 'char_19' 'char_20' 'char_21' 'char_22' 'char_23'\n",
      " 'char_24' 'char_25' 'char_26' 'char_27' 'char_28' 'char_29' 'char_30'\n",
      " 'char_31' 'char_32' 'char_33' 'char_34' 'char_35' 'char_36' 'char_37']\n",
      "\n",
      "Distinct DataTypes after processing: [dtype('O') dtype('int64')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have numeric, categorical (Object), and Boolean features in the dataset. Boolean in Python represents a True \n",
    "or False value; we need to convert this into numeric (1 and 0) for the model to process the data.\n",
    "\"\"\"\n",
    "# Create a temp dataset with the datatype of columns\n",
    "temp = pd.DataFrame(merged_df.dtypes)\n",
    "temp.columns = ['DataType']\n",
    "\n",
    "# Create a list with names of all Boolean columns\n",
    "boolean_columns = temp.index[temp['DataType'] == 'bool'].values\n",
    "print('Boolean columns - \\n', boolean_columns)\n",
    "\n",
    "# Convert all boolean columns to Binary numeric values\n",
    "for column in boolean_columns:\n",
    "    merged_df[column] = np.where(merged_df[column] == True, 1, 0)\n",
    "\n",
    "print('\\nDistinct DataTypes after processing:', act_train.dtypes.unique())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activity_category column has :  7 distinct values\n",
      "activity_id column has :  2197291 distinct values\n",
      "people_id column has :  151295 distinct values\n",
      "activity_date column has :  411 distinct values\n",
      "activity_type column has :  6515 distinct values\n",
      "char_1 column has :  2 distinct values\n",
      "group_1 column has :  29899 distinct values\n",
      "char_2 column has :  3 distinct values\n",
      "date column has :  1196 distinct values\n",
      "char_3 column has :  43 distinct values\n",
      "char_4 column has :  25 distinct values\n",
      "char_5 column has :  9 distinct values\n",
      "char_6 column has :  7 distinct values\n",
      "char_7 column has :  25 distinct values\n",
      "char_8 column has :  8 distinct values\n",
      "char_9 column has :  9 distinct values\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let us now have a look at the categorical features. We will first do a sanity check to understand the number of\n",
    "distinct values in each of the categorical features. If there are categorical features where there are unusually\n",
    "high numbers of distinct values, we have to decide if we can really convert them to a one-hot encoded structure \n",
    "for further processing.\n",
    "\"\"\"\n",
    "# Extracting the object columns from the above dataframe\n",
    "categorical_columns = temp.index[temp['DataType'] == 'O'].values\n",
    "# Check the number of distinct values in each categorical column\n",
    "for column in categorical_columns:\n",
    "    print(column +' column has : ', str(len(merged_df[column].unique())) + ' distinct values')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data after create Date Features: (2197291, 56)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let’s first fix the date-related columns and then huddle with the remaining categorical columns. The following code\n",
    "snippet converts the date values to new features and then deletes the actual column.\n",
    "\"\"\"\n",
    "# Create date related features for 'date' in customer data\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "merged_df['Year'] = merged_df['date'].dt.year\n",
    "merged_df['Month'] = merged_df['date'].dt.month\n",
    "merged_df['Quarter'] = merged_df['date'].dt.quarter\n",
    "merged_df['Week'] = merged_df['date'].dt.week\n",
    "merged_df['WeekDay'] = merged_df['date'].dt.weekday\n",
    "merged_df['Day'] = merged_df['date'].dt.day\n",
    "\n",
    "# Create date related features for 'date' in activity data\n",
    "merged_df['activity_date'] = pd.to_datetime(merged_df['activity_date'])\n",
    "\n",
    "merged_df['Activity_Year'] = merged_df['activity_date'].dt.year\n",
    "merged_df['Activity_Month'] = merged_df['activity_date'].dt.month\n",
    "merged_df['Activity_Quarter'] = merged_df['activity_date'].dt.quarter\n",
    "merged_df['Activity_Week'] = merged_df['activity_date'].dt.week\n",
    "merged_df['Activity_WeekDay'] = merged_df['activity_date'].dt.weekday\n",
    "merged_df['Activity_Day'] = merged_df['activity_date'].dt.day\n",
    "\n",
    "# Delete the original date columns\n",
    "del(merged_df['date'])\n",
    "del(merged_df['activity_date'])\n",
    "\n",
    "print('Shape of data after create Date Features:', merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_category</th>\n",
       "      <th>outcome</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>people_id</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>char_1</th>\n",
       "      <th>group_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Week</th>\n",
       "      <th>WeekDay</th>\n",
       "      <th>Day</th>\n",
       "      <th>Activity_Year</th>\n",
       "      <th>Activity_Month</th>\n",
       "      <th>Activity_Quarter</th>\n",
       "      <th>Activity_Week</th>\n",
       "      <th>Activity_WeekDay</th>\n",
       "      <th>Activity_Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type 4</td>\n",
       "      <td>0</td>\n",
       "      <td>act2_1734928</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>type 76</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>type 2</td>\n",
       "      <td>0</td>\n",
       "      <td>act2_2434093</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>type 1</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type 2</td>\n",
       "      <td>0</td>\n",
       "      <td>act2_3404049</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>type 1</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type 2</td>\n",
       "      <td>0</td>\n",
       "      <td>act2_3651215</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>type 1</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>type 2</td>\n",
       "      <td>0</td>\n",
       "      <td>act2_4109017</td>\n",
       "      <td>ppl_100</td>\n",
       "      <td>type 1</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  activity_category  outcome   activity_id people_id activity_type  char_1  \\\n",
       "0            type 4        0  act2_1734928   ppl_100       type 76  type 2   \n",
       "1            type 2        0  act2_2434093   ppl_100        type 1  type 2   \n",
       "2            type 2        0  act2_3404049   ppl_100        type 1  type 2   \n",
       "3            type 2        0  act2_3651215   ppl_100        type 1  type 2   \n",
       "4            type 2        0  act2_4109017   ppl_100        type 1  type 2   \n",
       "\n",
       "       group_1  char_2  char_3  char_4  ... Quarter Week WeekDay Day  \\\n",
       "0  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "1  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "2  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "3  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "4  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "\n",
       "  Activity_Year  Activity_Month  Activity_Quarter  Activity_Week  \\\n",
       "0          2023               8                 3             34   \n",
       "1          2022               9                 3             39   \n",
       "2          2022               9                 3             39   \n",
       "3          2023               8                 3             31   \n",
       "4          2023               8                 3             34   \n",
       "\n",
       "   Activity_WeekDay  Activity_Day  \n",
       "0                 5            26  \n",
       "1                 1            27  \n",
       "2                 1            27  \n",
       "3                 4             4  \n",
       "4                 5            26  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show top 5 row\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  people_id activity_type   activity_id      group_1\n",
      "0   ppl_100       type 76  act2_1734928  group 17304\n",
      "1   ppl_100        type 1  act2_2434093  group 17304\n",
      "2   ppl_100        type 1  act2_3404049  group 17304\n",
      "3   ppl_100        type 1  act2_3651215  group 17304\n",
      "4   ppl_100        type 1  act2_4109017  group 17304\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let us now have a look at the remaining categorical columns, which have very high numbers of distinct values.\n",
    "\"\"\"\n",
    "print(merged_df[['people_id', 'activity_type', 'activity_id', 'group_1']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   people_id  activity_id  activity_type  group_1\n",
      "0      100.0    1734928.0             76    17304\n",
      "1      100.0    2434093.0              1    17304\n",
      "2      100.0    3404049.0              1    17304\n",
      "3      100.0    3651215.0              1    17304\n",
      "4      100.0    4109017.0              1    17304\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It seems that we can convert all of the preceding categorical columns into numeric by extracting the relevant \n",
    "numeric ID from each of them, since each of these columns has values in the form of someText_ someNumber. Rather \n",
    "than converting these categorical columns into a bloated one-hot encoded dataset, we can temporarily use them as\n",
    "numeric features. However, if the performance of the model doesn’t reach our desired expectations after several\n",
    "experiments, we might have to revisit these features and try our best to incorporate them differently. But for now,\n",
    "we can consider them as numeric features.\n",
    "\"\"\"\n",
    "# For people ID, we would need to extract values after '_'\n",
    "merged_df.people_id = merged_df.people_id.apply(lambda x: x.split('_')[1])\n",
    "merged_df.people_id = pd.to_numeric(merged_df.people_id)\n",
    "\n",
    "# For activity ID also, we would need to extract values after '_'\n",
    "merged_df.activity_id = merged_df.activity_id.apply(lambda x: x.split('_')[1])\n",
    "merged_df.activity_id = pd.to_numeric(merged_df.activity_id)\n",
    "\n",
    "# For group_1 , we would need to extract values after ''\n",
    "merged_df.group_1 = merged_df.group_1.apply(lambda x: x.split(' ')[1])\n",
    "merged_df.group_1 = pd.to_numeric(merged_df.group_1)\n",
    "\n",
    "# For activity_type , we would need to extract values after ''\n",
    "merged_df.activity_type = merged_df.activity_type.apply(lambda x: x.split(' ')[1])\n",
    "merged_df.activity_type = pd.to_numeric(merged_df.activity_type)\n",
    "\n",
    "# Double check the new values in the dataframe\n",
    "print(merged_df[['people_id', 'activity_id', 'activity_type', 'group_1']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "let’s convert the remaining categorical columns, which have relatively low numbers of distinct values, to one-hot\n",
    "encoded form and render the final consolidated dataset.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a function that will intake the raw dataframe and the column name and return a one hot encoded DF\n",
    "\"\"\"\n",
    "def create_ohe(df, col):\n",
    "    le = LabelEncoder()\n",
    "    a = le.fit_transform(df[col]).reshape(-1, 1)\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    column_names = [col + '_' + str(i) for i in le.classes_]\n",
    "    return (pd.DataFrame(ohe.fit_transform(a), columns=column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target column :\n",
      " ['outcome']\n",
      "\n",
      "Numeric columns :\n",
      " ['Activity_Day', 'activity_id', 'Year', 'Month', 'group_1', 'Activity_Week', 'Activity_WeekDay', 'WeekDay', 'Activity_Quarter', 'Day', 'char_38', 'Activity_Year', 'people_id', 'Week', 'Activity_Month', 'Quarter', 'activity_type']\n",
      "\n",
      "Categorical columns :\n",
      " ['activity_category' 'char_1' 'char_2' 'char_3' 'char_4' 'char_5' 'char_6'\n",
      " 'char_7' 'char_8' 'char_9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of final df after onehot encoding: (2197291, 155)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Since the above function converts the column, one at a time We create a loop to create the final dataset \n",
    "with all features.\n",
    "\"\"\"\n",
    "\n",
    "#Creating a temporary dataframe that hosts column datatypes\n",
    "temp = pd.DataFrame(merged_df.dtypes)\n",
    "temp.columns = [\"DataType\"]\n",
    "\n",
    "target = ['outcome']\n",
    "numeric_columns = list(set(temp.index[(temp.DataType == 'float64') \n",
    "                                      | (temp.DataType == 'int64')].values) - set(target))\n",
    "categorical_columns = temp.index[temp.DataType == \"O\"].values\n",
    "\n",
    "print(\"\\nTarget column :\\n\",target)\n",
    "print(\"\\nNumeric columns :\\n\",numeric_columns)\n",
    "print(\"\\nCategorical columns :\\n\",categorical_columns)\n",
    "\n",
    "temp = merged_df[numeric_columns]\n",
    "for column in categorical_columns:\n",
    "    temp_df = create_ohe(merged_df, column)\n",
    "    temp = pd.concat([temp, temp_df], axis=1)\n",
    "print('\\nShape of final df after onehot encoding:', temp.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finally, before we begin with the model development, we need to split our datasets into train, validation, and test.\n",
    "The following code snippet leverages the “train_test_split” from the sklearn package in Python to split the final\n",
    "dataset created in the preceding into train and test, and then further divide the train into train and validation.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (1582048, 155)\n",
      "Shape of x_test: (439459, 155)\n",
      "Shape of x_val: (175784, 155)\n",
      "Shape of y_train: (1582048, 1)\n",
      "Shape of y_test: (439459, 1)\n",
      "Shape of y_val: (175784, 1)\n"
     ]
    }
   ],
   "source": [
    "# split the final dataset into train and test with 80:20\n",
    "x_train, x_test, y_train, y_test = train_test_split(temp, merged_df[target], test_size=0.2, random_state=2018)\n",
    "\n",
    "# split the train dataset further into train and validation with 90:10\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=2018)\n",
    "\n",
    "# Check the shape of each new dataset created\n",
    "print(\"Shape of x_train:\",x_train.shape)\n",
    "print(\"Shape of x_test:\",x_test.shape)\n",
    "print(\"Shape of x_val:\",x_val.shape)\n",
    "print(\"Shape of y_train:\",y_train.shape)\n",
    "print(\"Shape of y_test:\",y_test.shape)\n",
    "print(\"Shape of y_val:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For now, we have the training data in the desired form for building DL models for classification. We need to define\n",
    "a baseline benchmark that will help us set the threshold performance we should expect from our models\n",
    "for them to be considered useful and acceptable.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
