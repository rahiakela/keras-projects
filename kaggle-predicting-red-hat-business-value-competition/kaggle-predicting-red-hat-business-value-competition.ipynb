{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#       Predicting Red Hat Business Value \n",
    "#############################################################\n",
    "\"\"\"\n",
    "Reference: https://www.kaggle.com/c/predicting-red-hat-business-value\n",
    "\n",
    "The organization is an American multinational software company that provides open source software products to the \n",
    "enterprise community.Their primary product is Red Hat Enterprise Linux, the most popular distribution of Linux OS, \n",
    "used by various large enterprises. In its services, it helps organizations align their IT strategies by providing \n",
    "enterprise-grade solutions through an open business model and an affordable, predictable subscription model. \n",
    "These subscriptions from large enterprise customers create a substantial part of their revenue, and therefore it is \n",
    "of paramount importance for them to understand their valuable customers and serve them better by prioritizing \n",
    "resources and strategies to drive improved business value.\n",
    "\n",
    "How Can We Identify a Potential Customer?\n",
    "Red Hat has been in existence for over 25 years. In the long stint of business, they have accumulated and captured \n",
    "a vast amount of data from customer interactions and their descriptive attributes. This rich source of data could \n",
    "be a gold mine of patterns that can help in identifying a potential customer by studying the vast and complex \n",
    "historical patterns in the interaction data.\n",
    "With the ever-growing popularity and prowess of DL, we can develop a DNN that can learn from historic customer \n",
    "attributes and operational interaction data to understand the deep patterns and predict whether a new customer will\n",
    "potentially be a high-value customer for various business services.\n",
    "Therefore, we will develop and train a DNN to learn the chances that a customer will be a potential high-value \n",
    "customer, using various customer attributes and operational interaction attributes.\n",
    "\"\"\"\n",
    "# Exploring the Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 2 datasets provided in the Zip Folder\n",
    "act_train = pd.read_csv('D:\\\\ml-data\\\\predicting-red-hat-business-value\\\\act_train.csv')\n",
    "people = pd.read_csv('D:\\\\ml-data\\\\predicting-red-hat-business-value\\\\people.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DF: (2197291, 15)\n",
      "Shape of People DF: (189118, 41)\n"
     ]
    }
   ],
   "source": [
    "# Explore the shape of the datasets\n",
    "print('Shape of DF:', act_train.shape)\n",
    "print('Shape of People DF:', people.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>date</th>\n",
       "      <th>activity_category</th>\n",
       "      <th>char_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>char_5</th>\n",
       "      <th>char_6</th>\n",
       "      <th>char_7</th>\n",
       "      <th>char_8</th>\n",
       "      <th>char_9</th>\n",
       "      <th>char_10</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_1734928</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_2434093</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_3404049</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_3651215</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_4109017</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  people_id   activity_id        date activity_category char_1 char_2 char_3  \\\n",
       "0   ppl_100  act2_1734928  2023-08-26            type 4    NaN    NaN    NaN   \n",
       "1   ppl_100  act2_2434093  2022-09-27            type 2    NaN    NaN    NaN   \n",
       "2   ppl_100  act2_3404049  2022-09-27            type 2    NaN    NaN    NaN   \n",
       "3   ppl_100  act2_3651215  2023-08-04            type 2    NaN    NaN    NaN   \n",
       "4   ppl_100  act2_4109017  2023-08-26            type 2    NaN    NaN    NaN   \n",
       "\n",
       "  char_4 char_5 char_6 char_7 char_8 char_9  char_10  outcome  \n",
       "0    NaN    NaN    NaN    NaN    NaN    NaN  type 76        0  \n",
       "1    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "2    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "3    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "4    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the contents of the first dataset\n",
    "act_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people_id                  0\n",
       "activity_id                0\n",
       "date                       0\n",
       "activity_category          0\n",
       "char_1               2039676\n",
       "char_2               2039676\n",
       "char_3               2039676\n",
       "char_4               2039676\n",
       "char_5               2039676\n",
       "char_6               2039676\n",
       "char_7               2039676\n",
       "char_8               2039676\n",
       "char_9               2039676\n",
       "char_10               157615\n",
       "outcome                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exploring the contents of the training dataset, we can see that it mostly has customer interaction data but is \n",
    "completely anonymized. Given the confidentiality of customers and their attributes, the entire data is anonymized,\n",
    "and this leaves us with little knowledge about its true nature. This is a common problem in data science. Quite \n",
    "often, the team that develops DL models faces the challenge of the data confidentiality of the end customer and is \n",
    "therefore provided only anonymized and sometimes encrypted data. This still shouldn’t be a roadblock. It is \n",
    "definitely best to have a data dictionary and complete understanding of the dataset, but nevertheless, we can still\n",
    "develop models with the provided information.\n",
    "\"\"\"\n",
    "# Calculating the % of Null values in each column for activity data\n",
    "act_train.isnull().sum()  # show sum of null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2197291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_train.shape[0]  # show total row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people_id            0.000000\n",
       "activity_id          0.000000\n",
       "date                 0.000000\n",
       "activity_category    0.000000\n",
       "char_1               0.928268\n",
       "char_2               0.928268\n",
       "char_3               0.928268\n",
       "char_4               0.928268\n",
       "char_5               0.928268\n",
       "char_6               0.928268\n",
       "char_7               0.928268\n",
       "char_8               0.928268\n",
       "char_9               0.928268\n",
       "char_10              0.071732\n",
       "outcome              0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate percentage of null by dividing total null by total row count\n",
    "act_train.isnull().sum() / act_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>char_1</th>\n",
       "      <th>group_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>date</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>char_5</th>\n",
       "      <th>char_6</th>\n",
       "      <th>char_7</th>\n",
       "      <th>...</th>\n",
       "      <th>char_29</th>\n",
       "      <th>char_30</th>\n",
       "      <th>char_31</th>\n",
       "      <th>char_32</th>\n",
       "      <th>char_33</th>\n",
       "      <th>char_34</th>\n",
       "      <th>char_35</th>\n",
       "      <th>char_36</th>\n",
       "      <th>char_37</th>\n",
       "      <th>char_38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100002</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 8688</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>type 28</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100003</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 33592</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>type 4</td>\n",
       "      <td>type 8</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppl_100004</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 22593</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>type 40</td>\n",
       "      <td>type 25</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 4</td>\n",
       "      <td>type 16</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl_100006</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 6534</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>type 40</td>\n",
       "      <td>type 25</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 8</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    people_id  char_1      group_1  char_2        date   char_3   char_4  \\\n",
       "0     ppl_100  type 2  group 17304  type 2  2021-06-29   type 5   type 5   \n",
       "1  ppl_100002  type 2   group 8688  type 3  2021-01-06  type 28   type 9   \n",
       "2  ppl_100003  type 2  group 33592  type 3  2022-06-10   type 4   type 8   \n",
       "3  ppl_100004  type 2  group 22593  type 3  2022-07-20  type 40  type 25   \n",
       "4  ppl_100006  type 2   group 6534  type 3  2022-07-27  type 40  type 25   \n",
       "\n",
       "   char_5  char_6   char_7  ... char_29 char_30  char_31  char_32  char_33  \\\n",
       "0  type 5  type 3  type 11  ...   False    True     True    False    False   \n",
       "1  type 5  type 3  type 11  ...   False    True     True     True     True   \n",
       "2  type 5  type 2   type 5  ...   False   False     True     True     True   \n",
       "3  type 9  type 4  type 16  ...    True    True     True     True     True   \n",
       "4  type 9  type 3   type 8  ...   False   False     True    False    False   \n",
       "\n",
       "   char_34  char_35  char_36  char_37  char_38  \n",
       "0     True     True     True    False       36  \n",
       "1     True     True     True    False       76  \n",
       "2     True    False     True     True       99  \n",
       "3     True     True     True     True       76  \n",
       "4    False     True     True    False       84  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Around nine features have more than 90% null values. We can’t do much to fix these features. Let’s move ahead\n",
    "and have a look at the people dataset.\n",
    "\"\"\"\n",
    "# Explore the contents of People dataset\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let’s check how many missing data points the customer dataset has. Since the customer dataset has around 40+ \n",
    "features, we can combine the missing value percentages for all columns together with the preceding code, \n",
    "instead of looking at each column individually.\n",
    "\"\"\"\n",
    "# Calculate the % of null values in for the entire dataset\n",
    "people.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to remove: ['char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9']\n",
      "Shape of DF: (2197291, 6)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "And we see that none of the columns in the customer dataset has missing values.\n",
    "\n",
    "To create a consolidated dataset, we need to join the activity and customer data on the people_id key. But before\n",
    "we do that, we need to take care of a few things. We need to drop the columns in the activity data that have 90%\n",
    "missing values, as they cannot be fixed. Secondly, the “date” and “char_10” columns are present in both datasets.\n",
    "In order to avoid a name clash, let us rename the “date” column in the activity dataset to “activity_date” and \n",
    "“char_10” in the activity data as “activity_type.” Next, we also need to fix the missing values in the \n",
    "“activity_type” column. Once these two tasks are accomplished, we will join the two datasets and explore the\n",
    "consolidated data.\n",
    "\"\"\"\n",
    "# Create the list of columns to drop from activity data\n",
    "columns_to_remove = ['char_' + str(x) for x in np.arange(1, 10)]\n",
    "print('Columns to remove:', columns_to_remove)\n",
    "\n",
    "# Remove the columns from the activity data\n",
    "act_train = act_train[list(set(act_train.columns) - set(columns_to_remove))]\n",
    "\n",
    "# Rename the 2 columns to avoid name clashes in merged data\n",
    "act_train = act_train.rename(columns={'date': 'activity_date', 'char_10': 'activity_type'})\n",
    "\n",
    "# Replace nulls in the activity_type column with the mode\n",
    "act_train['activity_type'] = act_train['activity_type'].fillna(act_train['activity_type'].mode()[0])\n",
    "\n",
    "# Print the shape of the final activity dataset\n",
    "print('Shape of DF:', act_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before merging: (2197291, 6)\n",
      "Shape after merging : (2197291, 46)\n"
     ]
    }
   ],
   "source": [
    "# We can now join the two datasets to create a consolidate activity and customer attributes dataset.\n",
    "# Merge the 2 datasets on 'people_id' key\n",
    "merged_df = act_train.merge(people, on=['people_id'], how='inner')\n",
    "print('Shape before merging:', act_train.shape)\n",
    "print('Shape after merging :', merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for outcome: [0 1]\n",
      "\n",
      "Percentage of distribution for outcome-\n",
      "0    0.556046\n",
      "1    0.443954\n",
      "Name: outcome, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let us now study the target (i.e., the variable we want to predict), named “outcome” in the dataset. We can check \n",
    "the distribution between potential vs. nonpotential customers.\n",
    "\"\"\"\n",
    "print('Unique values for outcome:', merged_df['outcome'].unique())\n",
    "print('\\nPercentage of distribution for outcome-')\n",
    "print(merged_df['outcome'].value_counts() / merged_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct DataTypes:  [dtype('O'), dtype('int64'), dtype('bool')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can see that there is a good mix in the distribution of potential customers, as around 45% are potential \n",
    "customers.\n",
    "\"\"\"\n",
    "\n",
    "###########################################\n",
    "#        Data Engineering\n",
    "###########################################\n",
    "\"\"\"\n",
    "Next, given that we have 45 columns altogether to explore and transform, let’s expedite the process by automating\n",
    "a few things.\n",
    "\"\"\"\n",
    "# Checking the distinct datatypes in the dataset.\n",
    "print('Distinct DataTypes: ', list(merged_df.dtypes.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean columns - \n",
      " ['char_10' 'char_11' 'char_12' 'char_13' 'char_14' 'char_15' 'char_16'\n",
      " 'char_17' 'char_18' 'char_19' 'char_20' 'char_21' 'char_22' 'char_23'\n",
      " 'char_24' 'char_25' 'char_26' 'char_27' 'char_28' 'char_29' 'char_30'\n",
      " 'char_31' 'char_32' 'char_33' 'char_34' 'char_35' 'char_36' 'char_37']\n",
      "\n",
      "Distinct DataTypes after processing: [dtype('O') dtype('int64')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We have numeric, categorical (Object), and Boolean features in the dataset. Boolean in Python represents a True \n",
    "or False value; we need to convert this into numeric (1 and 0) for the model to process the data.\n",
    "\"\"\"\n",
    "# Create a temp dataset with the datatype of columns\n",
    "temp = pd.DataFrame(merged_df.dtypes)\n",
    "temp.columns = ['DataType']\n",
    "\n",
    "# Create a list with names of all Boolean columns\n",
    "boolean_columns = temp.index[temp['DataType'] == 'bool'].values\n",
    "print('Boolean columns - \\n', boolean_columns)\n",
    "\n",
    "# Convert all boolean columns to Binary numeric values\n",
    "for column in boolean_columns:\n",
    "    merged_df[column] = np.where(merged_df[column] == True, 1, 0)\n",
    "\n",
    "print('\\nDistinct DataTypes after processing:', act_train.dtypes.unique())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people_id column has :  151295 distinct values\n",
      "activity_category column has :  7 distinct values\n",
      "activity_id column has :  2197291 distinct values\n",
      "activity_date column has :  411 distinct values\n",
      "activity_type column has :  6515 distinct values\n",
      "char_1 column has :  2 distinct values\n",
      "group_1 column has :  29899 distinct values\n",
      "char_2 column has :  3 distinct values\n",
      "date column has :  1196 distinct values\n",
      "char_3 column has :  43 distinct values\n",
      "char_4 column has :  25 distinct values\n",
      "char_5 column has :  9 distinct values\n",
      "char_6 column has :  7 distinct values\n",
      "char_7 column has :  25 distinct values\n",
      "char_8 column has :  8 distinct values\n",
      "char_9 column has :  9 distinct values\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let us now have a look at the categorical features. We will first do a sanity check to understand the number of\n",
    "distinct values in each of the categorical features. If there are categorical features where there are unusually\n",
    "high numbers of distinct values, we have to decide if we can really convert them to a one-hot encoded structure \n",
    "for further processing.\n",
    "\"\"\"\n",
    "# Extracting the object columns from the above dataframe\n",
    "categorical_columns = temp.index[temp['DataType'] == 'O'].values\n",
    "# Check the number of distinct values in each categorical column\n",
    "for column in categorical_columns:\n",
    "    print(column +' column has : ', str(len(merged_df[column].unique())) + ' distinct values')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data after create Date Features: (2197291, 56)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let’s first fix the date-related columns and then huddle with the remaining categorical columns. The following code\n",
    "snippet converts the date values to new features and then deletes the actual column.\n",
    "\"\"\"\n",
    "# Create date related features for 'date' in customer data\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "merged_df['Year'] = merged_df['date'].dt.year\n",
    "merged_df['Month'] = merged_df['date'].dt.month\n",
    "merged_df['Quarter'] = merged_df['date'].dt.quarter\n",
    "merged_df['Week'] = merged_df['date'].dt.week\n",
    "merged_df['WeekDay'] = merged_df['date'].dt.weekday\n",
    "merged_df['Day'] = merged_df['date'].dt.day\n",
    "\n",
    "# Create date related features for 'date' in activity data\n",
    "merged_df['activity_date'] = pd.to_datetime(merged_df['activity_date'])\n",
    "\n",
    "merged_df['Activity_Year'] = merged_df['activity_date'].dt.year\n",
    "merged_df['Activity_Month'] = merged_df['activity_date'].dt.month\n",
    "merged_df['Activity_Quarter'] = merged_df['activity_date'].dt.quarter\n",
    "merged_df['Activity_Week'] = merged_df['activity_date'].dt.week\n",
    "merged_df['Activity_WeekDay'] = merged_df['activity_date'].dt.weekday\n",
    "merged_df['Activity_Day'] = merged_df['activity_date'].dt.day\n",
    "\n",
    "# Delete the original date columns\n",
    "del(merged_df['date'])\n",
    "del(merged_df['activity_date'])\n",
    "\n",
    "print('Shape of data after create Date Features:', merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>outcome</th>\n",
       "      <th>activity_category</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>char_1</th>\n",
       "      <th>group_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Week</th>\n",
       "      <th>WeekDay</th>\n",
       "      <th>Day</th>\n",
       "      <th>Activity_Year</th>\n",
       "      <th>Activity_Month</th>\n",
       "      <th>Activity_Quarter</th>\n",
       "      <th>Activity_Week</th>\n",
       "      <th>Activity_WeekDay</th>\n",
       "      <th>Activity_Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>0</td>\n",
       "      <td>type 4</td>\n",
       "      <td>act2_1734928</td>\n",
       "      <td>type 76</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>0</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_2434093</td>\n",
       "      <td>type 1</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>0</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_3404049</td>\n",
       "      <td>type 1</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>0</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_3651215</td>\n",
       "      <td>type 1</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>0</td>\n",
       "      <td>type 2</td>\n",
       "      <td>act2_4109017</td>\n",
       "      <td>type 1</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  people_id  outcome activity_category   activity_id activity_type  char_1  \\\n",
       "0   ppl_100        0            type 4  act2_1734928       type 76  type 2   \n",
       "1   ppl_100        0            type 2  act2_2434093        type 1  type 2   \n",
       "2   ppl_100        0            type 2  act2_3404049        type 1  type 2   \n",
       "3   ppl_100        0            type 2  act2_3651215        type 1  type 2   \n",
       "4   ppl_100        0            type 2  act2_4109017        type 1  type 2   \n",
       "\n",
       "       group_1  char_2  char_3  char_4  ... Quarter Week WeekDay Day  \\\n",
       "0  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "1  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "2  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "3  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "4  group 17304  type 2  type 5  type 5  ...       2   26       1  29   \n",
       "\n",
       "  Activity_Year  Activity_Month  Activity_Quarter  Activity_Week  \\\n",
       "0          2023               8                 3             34   \n",
       "1          2022               9                 3             39   \n",
       "2          2022               9                 3             39   \n",
       "3          2023               8                 3             31   \n",
       "4          2023               8                 3             34   \n",
       "\n",
       "   Activity_WeekDay  Activity_Day  \n",
       "0                 5            26  \n",
       "1                 1            27  \n",
       "2                 1            27  \n",
       "3                 4             4  \n",
       "4                 5            26  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show top 5 row\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  people_id activity_type   activity_id      group_1\n",
      "0   ppl_100       type 76  act2_1734928  group 17304\n",
      "1   ppl_100        type 1  act2_2434093  group 17304\n",
      "2   ppl_100        type 1  act2_3404049  group 17304\n",
      "3   ppl_100        type 1  act2_3651215  group 17304\n",
      "4   ppl_100        type 1  act2_4109017  group 17304\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let us now have a look at the remaining categorical columns, which have very high numbers of distinct values.\n",
    "\"\"\"\n",
    "print(merged_df[['people_id', 'activity_type', 'activity_id', 'group_1']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   people_id  activity_id  activity_type  group_1\n",
      "0      100.0    1734928.0             76    17304\n",
      "1      100.0    2434093.0              1    17304\n",
      "2      100.0    3404049.0              1    17304\n",
      "3      100.0    3651215.0              1    17304\n",
      "4      100.0    4109017.0              1    17304\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It seems that we can convert all of the preceding categorical columns into numeric by extracting the relevant \n",
    "numeric ID from each of them, since each of these columns has values in the form of someText_ someNumber. Rather \n",
    "than converting these categorical columns into a bloated one-hot encoded dataset, we can temporarily use them as\n",
    "numeric features. However, if the performance of the model doesn’t reach our desired expectations after several\n",
    "experiments, we might have to revisit these features and try our best to incorporate them differently. But for now,\n",
    "we can consider them as numeric features.\n",
    "\"\"\"\n",
    "# For people ID, we would need to extract values after '_'\n",
    "merged_df.people_id = merged_df.people_id.apply(lambda x: x.split('_')[1])\n",
    "merged_df.people_id = pd.to_numeric(merged_df.people_id)\n",
    "\n",
    "# For activity ID also, we would need to extract values after '_'\n",
    "merged_df.activity_id = merged_df.activity_id.apply(lambda x: x.split('_')[1])\n",
    "merged_df.activity_id = pd.to_numeric(merged_df.activity_id)\n",
    "\n",
    "# For group_1 , we would need to extract values after ''\n",
    "merged_df.group_1 = merged_df.group_1.apply(lambda x: x.split(' ')[1])\n",
    "merged_df.group_1 = pd.to_numeric(merged_df.group_1)\n",
    "\n",
    "# For activity_type , we would need to extract values after ''\n",
    "merged_df.activity_type = merged_df.activity_type.apply(lambda x: x.split(' ')[1])\n",
    "merged_df.activity_type = pd.to_numeric(merged_df.activity_type)\n",
    "\n",
    "# Double check the new values in the dataframe\n",
    "print(merged_df[['people_id', 'activity_id', 'activity_type', 'group_1']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "let’s convert the remaining categorical columns, which have relatively low numbers of distinct values, to one-hot\n",
    "encoded form and render the final consolidated dataset.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define a function that will intake the raw dataframe and the column name and return a one hot encoded DF\n",
    "\"\"\"\n",
    "def create_ohe(df, col):\n",
    "    le = LabelEncoder()\n",
    "    a = le.fit_transform(df[col]).reshape(-1, 1)\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    column_names = [col + '_' + str(i) for i in le.classes_]\n",
    "    return (pd.DataFrame(ohe.fit_transform(a), columns=column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target column :\n",
      " ['outcome']\n",
      "\n",
      "Numeric columns :\n",
      " ['people_id', 'Month', 'char_38', 'Activity_Week', 'Year', 'Activity_Month', 'Activity_WeekDay', 'Day', 'group_1', 'Week', 'Activity_Day', 'WeekDay', 'activity_id', 'Quarter', 'activity_type', 'Activity_Quarter', 'Activity_Year']\n",
      "\n",
      "Categorical columns :\n",
      " ['activity_category' 'char_1' 'char_2' 'char_3' 'char_4' 'char_5' 'char_6'\n",
      " 'char_7' 'char_8' 'char_9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of final df after onehot encoding: (2197291, 155)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Since the above function converts the column, one at a time We create a loop to create the final dataset \n",
    "with all features.\n",
    "\"\"\"\n",
    "\n",
    "#Creating a temporary dataframe that hosts column datatypes\n",
    "temp = pd.DataFrame(merged_df.dtypes)\n",
    "temp.columns = [\"DataType\"]\n",
    "\n",
    "target = ['outcome']\n",
    "numeric_columns = list(set(temp.index[(temp.DataType == 'float64') \n",
    "                                      | (temp.DataType == 'int64')].values) - set(target))\n",
    "categorical_columns = temp.index[temp.DataType == \"O\"].values\n",
    "\n",
    "print(\"\\nTarget column :\\n\",target)\n",
    "print(\"\\nNumeric columns :\\n\",numeric_columns)\n",
    "print(\"\\nCategorical columns :\\n\",categorical_columns)\n",
    "\n",
    "temp = merged_df[numeric_columns]\n",
    "for column in categorical_columns:\n",
    "    temp_df = create_ohe(merged_df, column)\n",
    "    temp = pd.concat([temp, temp_df], axis=1)\n",
    "print('\\nShape of final df after onehot encoding:', temp.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (1582048, 155)\n",
      "Shape of x_test: (439459, 155)\n",
      "Shape of x_val: (175784, 155)\n",
      "Shape of y_train: (1582048, 1)\n",
      "Shape of y_test: (439459, 1)\n",
      "Shape of y_val: (175784, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the final dataset into train and test with 80:20\n",
    "x_train, x_test, y_train, y_test = train_test_split(temp, merged_df[target], test_size=0.2, random_state=2018)\n",
    "\n",
    "# split the train dataset further into train and validation with 90:10\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=2018)\n",
    "\n",
    "# Check the shape of each new dataset created\n",
    "print(\"Shape of x_train:\",x_train.shape)\n",
    "print(\"Shape of x_test:\",x_test.shape)\n",
    "print(\"Shape of x_val:\",x_val.shape)\n",
    "print(\"Shape of y_train:\",y_train.shape)\n",
    "print(\"Shape of y_test:\",y_test.shape)\n",
    "print(\"Shape of y_val:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.556046\n",
       "1    0.443954\n",
       "Name: outcome, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For now, we have the training data in the desired form for building DL models for classification. We need to define\n",
    "a baseline benchmark that will help us set the threshold performance we should expect from our models\n",
    "for them to be considered useful and acceptable.\n",
    "\"\"\"\n",
    "\n",
    "##################################################\n",
    "#          Model Baseline Accuracy\n",
    "##################################################\n",
    "\"\"\"\n",
    "For all supervised classification use cases, our target variable would be a binary or multiclass (more than two\n",
    "classes) outcome. In our use case, we have the outcome as either 0 or 1. To validate the usefulness of a model, we\n",
    "should compare the result to what would have happened if we never had a model. In that case, we would make the\n",
    "largest class as the prediction for all customers and check what the accuracy looks like.\n",
    "If you remember, the target in our use case (i.e., the outcome variable) has a good distribution of 1’s and 0’s.\n",
    "Here is the distribution of the outcome variable between 1 and 0.\n",
    "\"\"\"\n",
    "# Checking the distribution of values in the target\n",
    "merged_df['outcome'].value_counts() / merged_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 101s 64us/step - loss: 8.3087 - acc: 0.4804 - val_loss: 8.8394 - val_acc: 0.4455\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 133s 84us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 118s 75us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1990c8be9b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "So, with the preceding distribution, we can say that if we do not have any model and make all predictions as 0\n",
    "(the largest class)—that is, predicting that none of the customers are potential high-value customers—then we\n",
    "would end up with at least 55.6% accuracy either way. This is our baseline accuracy. If we build a model that\n",
    "delivers us an overall accuracy anywhere below our benchmark, then it would be of practically no use.\n",
    "\"\"\"\n",
    "\n",
    "##################################################\n",
    "#       Designing the DNN for Classification\n",
    "##################################################\n",
    "\"\"\"\n",
    "For this use case, we have somewhat larger datasets. The training process might be more time-consuming than that\n",
    "of the regression use case. To save our time and be able to quickly get a well-functioning architecture in\n",
    "place, we will use a simple strategy. We will start with just three epochs for each kind of network we will\n",
    "experiment with, and once we find promising results, we will retrain the best architecture with the desired number\n",
    "of epochs for improved results.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Rule 1: Start small\n",
    "The following code snippet builds a DNN with just one layer and 256 neurons. We have used binary_crossentropy\n",
    "(since this a binary classification problem) as the loss function and accuracy as the metric to monitor. For\n",
    "classification problems, we can use several other metrics available within Keras, but accuracy is simple and fairly\n",
    "straightforward to comprehend. We will train the network for just three epochs and keep monitoring the loss as \n",
    "well as the accuracy on the training and validation dataset. If we don’t see promising results, we might have to\n",
    "try a new architecture.\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Design the deep neural network [Small + 1 layer] architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # activation=sigmoid for binary classification\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 183s 115us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 160s 101us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 163s 103us/step - loss: 8.8669 - acc: 0.4438 - val_loss: 8.8394 - val_acc: 0.4455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1990d2f1ef0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If you closely observe the results from the training output, you will see that the overall accuracy for training\n",
    "as well as validation datasets was around 0.44 (44%), which is way lower than our baseline accuracy. We can\n",
    "therefore conclude that training this model further might not be a fruitful idea.\n",
    "Let’s try a deeper network for the same number of neurons. So, we keep everything the same but add one more layer\n",
    "with the same number of neurons.\n",
    "\"\"\"\n",
    "# Design the deep neural network [Small + 2 layers]\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # activation=sigmoid for binary classification\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 156s 99us/step - loss: 7.1583 - acc: 0.5559 - val_loss: 7.1813 - val_acc: 0.5545\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 148s 93us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 140s 89us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1990f957be0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Again, as we can see, the initial results are not at all promising. The training and validation accuracy from the\n",
    "deeper network are not anywhere close to what we would expect. Instead of trying another deeper network\n",
    "with, say, three to five layers, let us try training with a bigger (medium-sized) network. We shall use a new\n",
    "architecture with just one layer but 512 neurons this time. Let us again train for three epochs and have a look\n",
    "at the metrics to check whether it is in line with what we would expect.\n",
    "\"\"\"\n",
    "# Design the deep neural network [Medium + 1 layers]\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 427s 270us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 414s 262us/step - loss: 7.1534 - acc: 0.5562 - val_loss: 7.1813 - val_acc: 0.5545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1990f865eb8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The medium-sized network too returned disappointing results. The training and validation accuracy from the \n",
    "medium-sized network are not really close to what we would expect. Let’s now try increasing the depth for\n",
    "the medium-sized network to see if the results improve.\n",
    "\"\"\"\n",
    "# Design the deep neural network [Medium + 2 layers]\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Input =  [1 2 3 4 5 6 7 8 9]\n",
      "Output =\n",
      "  [array([-1.54919334]), array([-1.161895]), array([-0.77459667]), array([-0.38729833]), array([0.]), array([0.38729833]), array([0.77459667]), array([1.161895]), array([1.54919334])]\n",
      "Output Mean =  0.0\n",
      "Output Std Dev =  1.0\n",
      "\n",
      "After Inverse Transforming = \n",
      "  [array([1.]), array([2.]), array([3.]), array([4.]), array([5.]), array([6.]), array([7.]), array([8.]), array([9.])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can see that the results have improved, but only by just a bit. We see an accuracy of around 55% for the \n",
    "training and validation datasets, but these results are again not great so we have to revisit our dataset.\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "#            Revisiting the Data\n",
    "###################################################\n",
    "\"\"\"\n",
    "We can standardize the input data with a ‘Standardscaler’ or a ‘Minmaxscaler’ using Python’s sklearn package’s\n",
    "tools or we can explore the options to revisit the one-hot encoding exercise for the categorical features we\n",
    "encoded as numeric. From these two options, the easiest and the least time-consuming would be standardizing or\n",
    "normalizing the data.\n",
    "\"\"\"\n",
    "\n",
    "##################################################\n",
    "#   Standardize, Normalize, or Scale the Data\n",
    "##################################################\n",
    "\"\"\"\n",
    "it is a good practice to standardize or normalize the data before providing it as training data for the DL models.\n",
    "However, in our classification use case, we can see that the performance is very poor on the raw data. To improve\n",
    "our model performance, let us try standardizing our data. (Alternatively, you can normalize the data too.)\n",
    "In standardization, we transform the data into a form where the mean is 0 and the standard deviation is 1. The\n",
    "distribution of the data in this form is a great input candidate for our neuron’s activation function and\n",
    "therefore improves the ability to learn more appropriately.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a dummy input\n",
    "dummy_input = np.arange(1, 10)\n",
    "print('Dummy Input = ', dummy_input)\n",
    "\n",
    "# Create a standardscaler instance and fit the data\n",
    "scaler = StandardScaler()\n",
    "output = scaler.fit_transform(dummy_input.reshape(-1, 1))\n",
    "print('Output =\\n ', list(output))\n",
    "print('Output Mean = ', output.mean())\n",
    "print('Output Std Dev = ', output.std())\n",
    "print('\\nAfter Inverse Transforming = \\n ', list(scaler.inverse_transform(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\ipykernel_launcher.py:14: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\ipykernel_launcher.py:15: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  from ipykernel import kernelapp as app\n",
      "D:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#       Transforming the Input Data\n",
    "###############################################\n",
    "\"\"\"\n",
    "To transform the input data for the development of the model, please note that we should only use the training data\n",
    "to fit the scaler transformation and use the same fitted object to transform the validation and test input data.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# uses the x_train dataset to fit and transform the scaled values for all three datasets\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 157s 99us/step - loss: 0.2517 - acc: 0.8891 - val_loss: 0.2150 - val_acc: 0.9107\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 151s 95us/step - loss: 0.2016 - acc: 0.9183 - val_loss: 0.1876 - val_acc: 0.9259\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 142s 89us/step - loss: 0.1778 - acc: 0.9308 - val_loss: 0.1792 - val_acc: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1992b3cc668>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that we have the standard scaled datasets, we can provide this newly augmented data for training. \n",
    "Please note that we haven’t made any transformations to the labels or the target.\n",
    "\"\"\"\n",
    "\n",
    "####################################################\n",
    "#      DNNs for Classification with Improved Data\n",
    "####################################################\n",
    "\"\"\"\n",
    "Let us now start with a medium-sized network to see if we get improved results. \n",
    "We will start with just three epochs.\n",
    "\"\"\"\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Design the deep neural network [Medium + 2 layers]\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=x_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train_scaled, y_train, validation_data=(x_val_scaled, y_val), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439459/439459 [==============================] - 23s 52us/step\n",
      "Metric  loss : 0.18\n",
      "Metric  acc : 0.93\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can see the drastic improvement in the performance of the network in providing the standardized datasets.\n",
    "We have an almost 93% accuracy on the training and validation datasets. Let’s use this model to evaluate the\n",
    "model performance on the test datasets.\n",
    "\"\"\"\n",
    "result = model.evaluate(x_test_scaled, y_test)\n",
    "for i in range(len(model.metrics_names)):\n",
    "    print('Metric ', model.metrics_names[i], ':', str(round(result[i], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 425s 269us/step - loss: 0.2167 - acc: 0.9051 - val_loss: 0.1685 - val_acc: 0.9285\n",
      "Epoch 2/3\n",
      "1582048/1582048 [==============================] - 303s 191us/step - loss: 0.1401 - acc: 0.9435 - val_loss: 0.1258 - val_acc: 0.9502\n",
      "Epoch 3/3\n",
      "1582048/1582048 [==============================] - 332s 210us/step - loss: 0.1105 - acc: 0.9567 - val_loss: 0.1052 - val_acc: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1992e4c13c8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We see great results on the test dataset. Let’s try improving the architecture a bit and see. We can a medium-sized\n",
    "deeper network to see if the results are better than with the medium-sized network.\n",
    "\"\"\"\n",
    "# Designing the Deep Neural Network [Medium – 2 Layers]\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=x_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train_scaled, y_train, validation_data=(x_val_scaled, y_val), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/3\n",
      "1582048/1582048 [==============================] - 1070s 677us/step - loss: 0.2156 - acc: 0.9053 - val_loss: 0.1591 - val_acc: 0.9337\n",
      "Epoch 2/3\n",
      "1581184/1582048 [============================>.] - ETA: 0s - loss: 0.1077 - acc: 0.9578"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The training and validation accuracy has improved even further to 95%. This small increase with just 3 epochs is\n",
    "awesome. We can now be confident of the performance for the model with the architecture.\n",
    "let’s take a final shot with a larger and deeper network and see the results with 3 epochs. In case we see only\n",
    "small improvements, we will use the same architecture for 15 epochs and use the model for our final predictions.\n",
    "\"\"\"\n",
    "# Designing the network Deep Neural Network – [Large + 2 Layers]\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=x_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train_scaled, y_train, validation_data=(x_val_scaled, y_val), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1582048 samples, validate on 175784 samples\n",
      "Epoch 1/15\n",
      "1582048/1582048 [==============================] - 325s 206us/step - loss: 0.2156 - acc: 0.9058 - val_loss: 0.1615 - val_acc: 0.9330\n",
      "Epoch 2/15\n",
      " 742144/1582048 [=============>................] - ETA: 2:53 - loss: 0.1472 - acc: 0.9403"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-0b92f39af834>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \"\"\"\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ml-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We see an overall accuracy on the validation dataset as 96% and a similar score for the training dataset. So,\n",
    "there really isn’t a lot of improvement in the performance of the model due to increasing the size from a medium\n",
    "(512-neuron) to a larger (1024-neuron) architecture.\n",
    "let’s train a medium-sized (512-neuron) deep network with two layers for 15 epochs, look at the final training\n",
    "and validation accuracy, and then use the trained model to evaluate the test datasets.\n",
    "\"\"\"\n",
    "# Designing the network Deep Neural Network – [Medium + 2 Layers]\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=x_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train_scaled, y_train, validation_data=(x_val_scaled, y_val), epochs=15, batch_size=64)\n",
    "\n",
    "\"\"\"\n",
    "The final model with a medium-size architecture of 512 neurons and two layers gave great performance results on\n",
    "the training and validation datasets. We have an accuracy of ~98% for both datasets. Let us now\n",
    "validate the model performance on the test dataset.\n",
    "\"\"\"\n",
    "result = model.evaluate(x_test_scaled, y_test)\n",
    "for i in range(len(model.metrics_names)):\n",
    "    print('Metric ', model.metrics_names[i], ':', str(round(result[i], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The performance on the unseen test dataset is also great and consistent. Our model is performing really well on\n",
    "the test dataset. Let us have a look at the loss curve for the model, just like we did for the regression use \n",
    "case. We will plot the loss in each epoch (15 in total for this mode) for the training and validation datasets.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "plt.title('Model Training & Validation loss across epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We can see decreasing loss across both datasets. Similarly, let’s have a look at the accuracy metric during the\n",
    "model training. The accuracy metric for the training and validation datasets is also stored in the model history.\n",
    "\"\"\"\n",
    "plt.plot(model.history.history['acc'])\n",
    "plt.plot(model.history.history['val_acc'])\n",
    "plt.title(\"Model's Training & Validation Accuracy across epochs\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
