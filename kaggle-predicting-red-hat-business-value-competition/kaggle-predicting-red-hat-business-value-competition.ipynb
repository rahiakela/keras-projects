{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#       Predicting Red Hat Business Value \n",
    "#############################################################\n",
    "\"\"\"\n",
    "Reference: https://www.kaggle.com/c/predicting-red-hat-business-value\n",
    "\n",
    "The organization is an American multinational software company that provides open source software products to the \n",
    "enterprise community.Their primary product is Red Hat Enterprise Linux, the most popular distribution of Linux OS, \n",
    "used by various large enterprises. In its services, it helps organizations align their IT strategies by providing \n",
    "enterprise-grade solutions through an open business model and an affordable, predictable subscription model. \n",
    "These subscriptions from large enterprise customers create a substantial part of their revenue, and therefore it is \n",
    "of paramount importance for them to understand their valuable customers and serve them better by prioritizing \n",
    "resources and strategies to drive improved business value.\n",
    "\n",
    "How Can We Identify a Potential Customer?\n",
    "Red Hat has been in existence for over 25 years. In the long stint of business, they have accumulated and captured \n",
    "a vast amount of data from customer interactions and their descriptive attributes. This rich source of data could be \n",
    "a gold mine of patterns that can help in identifying a potential customer by studying the vast and complex historical\n",
    "patterns in the interaction data.\n",
    "With the ever-growing popularity and prowess of DL, we can develop a DNN that can learn from historic customer \n",
    "attributes and operational interaction data to understand the deep patterns and predict whether a new customer will\n",
    "potentially be a high-value customer for various business services.\n",
    "Therefore, we will develop and train a DNN to learn the chances that a customer will be a potential high-value \n",
    "customer, using various customer attributes and operational interaction attributes.\n",
    "\"\"\"\n",
    "# Exploring the Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 2 datasets provided in the Zip Folder\n",
    "act_train = pd.read_csv('D:\\\\ml-data\\\\predicting-red-hat-business-value\\\\act_train.csv')\n",
    "people = pd.read_csv('D:\\\\ml-data\\\\predicting-red-hat-business-value\\\\people.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DF: (2197291, 15)\n",
      "Shape of People DF: (189118, 41)\n"
     ]
    }
   ],
   "source": [
    "# Explore the shape of the datasets\n",
    "print('Shape of DF:', act_train.shape)\n",
    "print('Shape of People DF:', people.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>date</th>\n",
       "      <th>activity_category</th>\n",
       "      <th>char_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>char_5</th>\n",
       "      <th>char_6</th>\n",
       "      <th>char_7</th>\n",
       "      <th>char_8</th>\n",
       "      <th>char_9</th>\n",
       "      <th>char_10</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_1734928</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_2434093</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_3404049</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_3651215</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_4109017</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>type 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>type 1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  people_id   activity_id        date activity_category char_1 char_2 char_3  \\\n",
       "0   ppl_100  act2_1734928  2023-08-26            type 4    NaN    NaN    NaN   \n",
       "1   ppl_100  act2_2434093  2022-09-27            type 2    NaN    NaN    NaN   \n",
       "2   ppl_100  act2_3404049  2022-09-27            type 2    NaN    NaN    NaN   \n",
       "3   ppl_100  act2_3651215  2023-08-04            type 2    NaN    NaN    NaN   \n",
       "4   ppl_100  act2_4109017  2023-08-26            type 2    NaN    NaN    NaN   \n",
       "\n",
       "  char_4 char_5 char_6 char_7 char_8 char_9  char_10  outcome  \n",
       "0    NaN    NaN    NaN    NaN    NaN    NaN  type 76        0  \n",
       "1    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "2    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "3    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  \n",
       "4    NaN    NaN    NaN    NaN    NaN    NaN   type 1        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the contents of the first dataset\n",
    "act_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people_id                  0\n",
       "activity_id                0\n",
       "date                       0\n",
       "activity_category          0\n",
       "char_1               2039676\n",
       "char_2               2039676\n",
       "char_3               2039676\n",
       "char_4               2039676\n",
       "char_5               2039676\n",
       "char_6               2039676\n",
       "char_7               2039676\n",
       "char_8               2039676\n",
       "char_9               2039676\n",
       "char_10               157615\n",
       "outcome                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Exploring the contents of the training dataset, we can see that it mostly has customer interaction data but is \n",
    "completely anonymized. Given the confidentiality of customers and their attributes, the entire data is anonymized,\n",
    "and this leaves us with little knowledge about its true nature. This is a common problem in data science. Quite \n",
    "often, the team that develops DL models faces the challenge of the data confidentiality of the end customer and is \n",
    "therefore provided only anonymized and sometimes encrypted data. This still shouldn’t be a roadblock. It is \n",
    "definitely best to have a data dictionary and complete understanding of the dataset, but nevertheless, we can still\n",
    "develop models with the provided information.\n",
    "\"\"\"\n",
    "# Calculating the % of Null values in each column for activity data\n",
    "act_train.isnull().sum()  # show sum of null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2197291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_train.shape[0]  # show total row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people_id            0.000000\n",
       "activity_id          0.000000\n",
       "date                 0.000000\n",
       "activity_category    0.000000\n",
       "char_1               0.928268\n",
       "char_2               0.928268\n",
       "char_3               0.928268\n",
       "char_4               0.928268\n",
       "char_5               0.928268\n",
       "char_6               0.928268\n",
       "char_7               0.928268\n",
       "char_8               0.928268\n",
       "char_9               0.928268\n",
       "char_10              0.071732\n",
       "outcome              0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate percentage of null by dividing total null by total row count\n",
    "act_train.isnull().sum() / act_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>char_1</th>\n",
       "      <th>group_1</th>\n",
       "      <th>char_2</th>\n",
       "      <th>date</th>\n",
       "      <th>char_3</th>\n",
       "      <th>char_4</th>\n",
       "      <th>char_5</th>\n",
       "      <th>char_6</th>\n",
       "      <th>char_7</th>\n",
       "      <th>...</th>\n",
       "      <th>char_29</th>\n",
       "      <th>char_30</th>\n",
       "      <th>char_31</th>\n",
       "      <th>char_32</th>\n",
       "      <th>char_33</th>\n",
       "      <th>char_34</th>\n",
       "      <th>char_35</th>\n",
       "      <th>char_36</th>\n",
       "      <th>char_37</th>\n",
       "      <th>char_38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 17304</td>\n",
       "      <td>type 2</td>\n",
       "      <td>2021-06-29</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100002</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 8688</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>type 28</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 11</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100003</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 33592</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-06-10</td>\n",
       "      <td>type 4</td>\n",
       "      <td>type 8</td>\n",
       "      <td>type 5</td>\n",
       "      <td>type 2</td>\n",
       "      <td>type 5</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ppl_100004</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 22593</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>type 40</td>\n",
       "      <td>type 25</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 4</td>\n",
       "      <td>type 16</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl_100006</td>\n",
       "      <td>type 2</td>\n",
       "      <td>group 6534</td>\n",
       "      <td>type 3</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>type 40</td>\n",
       "      <td>type 25</td>\n",
       "      <td>type 9</td>\n",
       "      <td>type 3</td>\n",
       "      <td>type 8</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    people_id  char_1      group_1  char_2        date   char_3   char_4  \\\n",
       "0     ppl_100  type 2  group 17304  type 2  2021-06-29   type 5   type 5   \n",
       "1  ppl_100002  type 2   group 8688  type 3  2021-01-06  type 28   type 9   \n",
       "2  ppl_100003  type 2  group 33592  type 3  2022-06-10   type 4   type 8   \n",
       "3  ppl_100004  type 2  group 22593  type 3  2022-07-20  type 40  type 25   \n",
       "4  ppl_100006  type 2   group 6534  type 3  2022-07-27  type 40  type 25   \n",
       "\n",
       "   char_5  char_6   char_7  ... char_29 char_30  char_31  char_32  char_33  \\\n",
       "0  type 5  type 3  type 11  ...   False    True     True    False    False   \n",
       "1  type 5  type 3  type 11  ...   False    True     True     True     True   \n",
       "2  type 5  type 2   type 5  ...   False   False     True     True     True   \n",
       "3  type 9  type 4  type 16  ...    True    True     True     True     True   \n",
       "4  type 9  type 3   type 8  ...   False   False     True    False    False   \n",
       "\n",
       "   char_34  char_35  char_36  char_37  char_38  \n",
       "0     True     True     True    False       36  \n",
       "1     True     True     True    False       76  \n",
       "2     True    False     True     True       99  \n",
       "3     True     True     True     True       76  \n",
       "4    False     True     True    False       84  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Around nine features have more than 90% null values. We can’t do much to fix these features. Let’s move ahead\n",
    "and have a look at the people dataset.\n",
    "\"\"\"\n",
    "# Explore the contents of People dataset\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let’s check how many missing data points the customer dataset has. Since the customer dataset has around 40+ \n",
    "features, we can combine the missing value percentages for all columns together with the preceding code, \n",
    "instead of looking at each column individually.\n",
    "\"\"\"\n",
    "# Calculate the % of null values in for the entire dataset\n",
    "people.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to remove: ['char_1', 'char_2', 'char_3', 'char_4', 'char_5', 'char_6', 'char_7', 'char_8', 'char_9']\n",
      "Shape of DF: (2197291, 6)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "And we see that none of the columns in the customer dataset has missing values.\n",
    "\n",
    "To create a consolidated dataset, we need to join the activity and customer data on the people_id key. But before\n",
    "we do that, we need to take care of a few things. We need to drop the columns in the activity data that have 90%\n",
    "missing values, as they cannot be fixed. Secondly, the “date” and “char_10” columns are present in both datasets.\n",
    "In order to avoid a name clash, let us rename the “date” column in the activity dataset to “activity_date” and \n",
    "“char_10” in the activity data as “activity_type.” Next, we also need to fix the missing values in the \n",
    "“activity_type” column. Once these two tasks are accomplished, we will join the two datasets and explore the\n",
    "consolidated data.\n",
    "\"\"\"\n",
    "# Create the list of columns to drop from activity data\n",
    "columns_to_remove = ['char_' + str(x) for x in np.arange(1, 10)]\n",
    "print('Columns to remove:', columns_to_remove)\n",
    "\n",
    "# Remove the columns from the activity data\n",
    "act_train = act_train[list(set(act_train.columns) - set(columns_to_remove))]\n",
    "\n",
    "# Rename the 2 columns to avoid name clashes in merged data\n",
    "act_train = act_train.rename(columns={'date': 'activity_date', 'char_10': 'activity_type'})\n",
    "\n",
    "# Replace nulls in the activity_type column with the mode\n",
    "act_train['activity_type'] = act_train['activity_type'].fillna(act_train['activity_type'].mode()[0])\n",
    "\n",
    "# Print the shape of the final activity dataset\n",
    "print('Shape of DF:', act_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before merging: (2197291, 6)\n",
      "Shape after merging : (2197291, 46)\n"
     ]
    }
   ],
   "source": [
    "# We can now join the two datasets to create a consolidate activity and customer attributes dataset.\n",
    "# Merge the 2 datasets on 'people_id' key\n",
    "merged_df = act_train.merge(people, on=['people_id'], how='inner')\n",
    "print('Shape before merging:', act_train.shape)\n",
    "print('Shape after merging :', merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for outcome: [0 1]\n",
      "\n",
      "Percentage of distribution for outcome-\n",
      "0    0.556046\n",
      "1    0.443954\n",
      "Name: outcome, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let us now study the target (i.e., the variable we want to predict), named “outcome” in the dataset. We can check \n",
    "the distribution between potential vs. nonpotential customers.\n",
    "\"\"\"\n",
    "print('Unique values for outcome:', merged_df['outcome'].unique())\n",
    "print('\\nPercentage of distribution for outcome-')\n",
    "print(merged_df['outcome'].value_counts() / merged_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can see that there is a good mix in the distribution of potential customers, as around 45% are potential \n",
    "customers.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
